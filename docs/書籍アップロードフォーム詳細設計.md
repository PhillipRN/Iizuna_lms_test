# 書籍アップロードフォーム 詳細設計

`docs/書籍アップロードフォーム設計.md` を踏まえ、ローカルDocker→AWS複製環境で実装・検証するための詳細設計をまとめる。参照ドキュメントは `docs/開発ワークフローガイド.md`。`ReadMe.md` は本番用のため対象外。

## 1. 画面仕様
### 1.1 URL / 認可
- パス: `/il_admin/books/upload`
- 認可: 管理者ロール（`AdminLoginController` 経由）、CSRFトークン必須。

### 1.2 UI構成
1. **アップロードフォーム**
   - 入力項目
     - ファイル選択（複数可、`.xls`, `.xlsx` のみ、最大5件）
     - フォルダ識別子（初期値は日付 `YYYYMMDD`、任意上書き可）
     - ドライラン有無（チェックボックス）
     - 備考テキスト（任意、255文字）
   - 送信ボタン：`検証して反映`
2. **進捗表示モーダル**
   - ステップ: `構文チェック → converter → DB反映 → 完了`
   - サーバーからのステータスをpollingまたはSSEで取得。
3. **結果パネル**
   - 成功時: 処理ID、書き込まれたテーブル、CSV保存パス、ログIDを表示。
   - 失敗時: エラー概要 + 詳細ログダウンロードリンク。
4. **エラー一覧テーブル**
   - 列: 行番号 / シート名 / 列名 / メッセージ。
   - CSVでのエクスポートボタンを配置。

## 2. ルーティング & コントローラ
- `app/Routes/admin.php` に以下を追加：
  ```php
  $router->get('/books/upload', 'AdminBookUploadController@showForm');
  $router->post('/books/upload', 'AdminBookUploadController@handleUpload');
  $router->get('/books/upload/status/{jobId}', 'AdminBookUploadController@status');
  ```
- `AdminBookUploadController`
  - `showForm()`: フォーム表示。過去の処理履歴（最新10件）を取得して渡す。
  - `handleUpload()`: CSRF + 権限チェック後、`BookUploadService` を呼び出し処理IDを返す。非同期実行のためジョブキューを擬似的に実装（DBテーブル `book_upload_jobs` を使いステータス管理）。
  - `status($jobId)`: JSONで進捗を返し、フロントが定期的に取得。

## 3. データモデル
### 3.1 `book_upload_jobs` テーブル
| 列名 | 型 | 説明 |
| --- | --- | --- |
| id | BIGINT PK | ジョブID |
| job_uuid | CHAR(36) | API向けID |
| user_id | INT | 実行者ID（`Admin`）|
| folder_name | VARCHAR(32) | import/exportフォルダ名 |
| file_count | TINYINT | 処理ファイル数 |
| status | ENUM(pending, validating, converting, applying, success, failed) | 進捗 |
| is_dry_run | TINYINT | 1=ドライラン |
| log_path | TEXT | ログファイルパス |
| message | TEXT | エラー詳細 |
| created_at / updated_at | DATETIME | |

### 3.2 ログ
- `app/Logs/book_upload.log`: 1行1処理ID、JSONフォーマット `{timestamp, job_uuid, user_id, status, elapsed, details}`。

## 4. サービス層
`app/Services/BookUploadService.php`
- 依存: `PhpSpreadsheet`, `Symfony\Component\Process\Process`、`BookUploadValidator`, `BookUploadJobRepository`。
- 主なメソッド
  - `startUpload(array $files, string $folderName, bool $dryRun, string $memo): string`
    - 一時ディレクトリ `storage/uploads/books/tmp/<job_uuid>` を作成
    - ファイル移動後に `BookUploadValidator::validate($spreadsheet)` を実行
    - エラーがあれば例外を投げ、ファイル削除
    - 問題なければ `BookUploadPipeline` にジョブを渡して非同期実行
  - `getStatus(string $jobUuid)`
    - `book_upload_jobs` から進捗とログサマリを取得

## 5. バリデーション詳細
`app/Services/BookUploadValidator.php`
- シート存在チェック: `01_メタ情報`, `02_問題形式`, `03_配点` (例)
- 列定義: 設定ファイル `config/book_upload_columns.php` に配列で保持
- ルール例
  - `問題ID`: `/^TC\d{5}-Q\d{3}$/`
  - `配点`: 数値、0.5刻み
  - `正解`: 文字数1、A〜E
  - 配分比率: 行単位で合計100 (±0.1許容)
- エラー収集は `ValidationError` DTOに格納し、コントローラへ return

## 6. 処理パイプライン
`app/Jobs/BookUploadPipeline.php`
1. `converter.php` 実行
   ```php
   $process = new Process(['php', 'converter.php'], 'setup_database/iizuna_lms');
   ```
   - 環境変数 `IMPORT_FOLDER` に処理IDを渡し、対象ディレクトリのみ変換。
2. CSV確認
   - `setup_database/iizuna_lms/export/<処理ID>/` の存在と各TCディレクトリを検証。
3. DB反映
   ```bash
   ./auto_setup_with_directory_name.sh <処理ID>
   php app/Commands/setup_book_range.php --id=<処理ID>
   ```
4. ドライラン
   - `auto_setup_with_directory_name.sh --dry-run` オプションを追加し、SQLを `/tmp/book_upload/<処理ID>.sql` に書き出す。

## 7. エラーハンドリング
- validator例外 → HTTP 400（画面に即時表示）
- converter/auto_setup失敗 → ジョブステータス `failed`、標準出力とエラーを `log_path` に保存しダウンロード可能に。
- RDS接続失敗 → MySQLエラーコードに応じたメッセージ（例: 1045 認証失敗、2003 接続拒否）。
- どのケースもDBトランザクションは使わない（既存スクリプトが単発INSERTのため）。再実行はフォルダ名を新規にする。

## 8. テスト設計
- **ユニット**: `BookUploadValidatorTest`（シート欠落/列名不正/配点ズレ）、`BookUploadServiceTest`（エラー時の一時ファイル削除）
- **結合**: `BookUploadPipelineTest`（Processモックで正常/異常、ドライラン）
- **E2E**: Featureテストで `/books/upload` に対してSymfony HTTPクライアントからPOSTし、ステータス遷移を確認。
- ローカルDockerで `make test`、AWS複製環境で `./vendor/phpunit/phpunit/phpunit --filter BookUpload`

## 9. デプロイ / パラメータ
- 新規env/configキー: `BOOK_UPLOAD_MAX_FILES`, `BOOK_UPLOAD_TMP_PATH`, `BOOK_UPLOAD_LOG_PATH`。
- `composer.json` に `symfony/process` と `phpoffice/phpspreadsheet`（既に依存にあるためバージョン確認のみ）。
- `docker-compose.yml` 変更不要だが、ローカルの `/tmp/book_upload` を永続化する場合は volume を追加検討。

## 10. 運用ポイント
- 毎朝 `book_upload_jobs` のfailed件数をSlack通知（将来対応）。
- RDS側でAUTO_INCREMENTの肥大化を避けるため、処理済みジョブは90日後にアーカイブするバッチを後続タスクとして用意。
- 本番適用前に `docs/RDS自動反映準備手順.md` のChecklistを全て完了すること。
